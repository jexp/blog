== Klein aber oho - DuckDB als leichtgewichtige Analysedatenbank

:imagesdir: ../../img/

Ist es im Zeitalter von BigData denn noch möglich, Datenanalysen von hunderten von Gigabyte auf dem eigenen Rechner durchzuführen, ohne ihn in die Knie zu zwingen?
Ja, jetzt wieder, mittels DuckDB, dem neuesten Star in der Datenbankwelt.
Diese leichtgewichtige, hocheffiziente Datenbank kann leicht in die eigenen Python-, Java-, Swift- oder andere Programme integriert werden und macht moderne SQL-Analysefunktionalität einfach nutzbar.

Die verbreitetste Datenbank ist nicht etwa MySQL oder MS Access, sondern [SQLite], das in zehntausenden Anwendungen (mehr als eine Billion Datenbanken) in unseren Mobiltelefonen, Webbrowsern, Spielen, Fernsehern und anderen Geräten genutzt wird, um schnell und effizient strukturierte Daten abzulegen und mit SQL zugreifbar zu machen.
Der besondere Reiz liegt darin, dass SQLite als Bibliothek einfach in Programme integriert werden kann und dann innerhalb des eigenen Prozesses läuft, ohne einen separaten Server zu betreiben.
SQLite ist eine volltransaktionale OLTP Datenbank mit einer extrem guten Testabdeckung und Verfikation.
Im Podcast [SQLitePodcast] hat der Erfinder, Richard Hipp die Geschichte der Datenbank anschaulich dargelegt.

DuckDB ist der analytische OLAP Zwilling dazu, also "SQLite für Datenanalyse".

Von Hannes Mühleisen und Mark Raasveldt als Datenbank-Doktoranden am CWI (Centrum Wiskunde & Informatica) in Amsterdam entwickelt, hat DuckDB ihren Usprung in der Erkenntnis das Data-Scientists in R oft kein SQL benutzten, weil keine gute Datenbankintegration für R existierte und Client-Server Datenbanken unpraktisch waren.
Ausgehend von "wie schwer kann dass den sein", ist während nunmehr 8 Jahren Arbeit erst im universitären und jetzt auch im kommerziellen Umfeld ein beindruckendes Datenbanktool entstanden, dass Anwendern weit über R hinaus den Zugang zur effizienten Datenanalyse ermöglicht.

DuckDB ist in C++ 11 implementiert, hat keine weiteren Abhängigkeiten, und als quelloffene Software unter der MIT Lizenz veröffentlicht.

Oft assoziieren wir mit der Analyse von Datenmengen im zwei- oder dreistelligen Gigabytebereich, größere Spark-Cluster oder DWH Lösungen wie Snowflake, PowerBI, Cognos, Redshift oder ähnliches.
Jede davon mit den entsprechenden Investitionen in Infrastruktur, Zeit, Datentransfer und Komplexität.

De Gründer von Motherduck, Jordan Tigani einer der originalen Entwickler von Google's BigQuery hat dazu im Artikel  "Big Data is Dead" [BigData] Stellung genommen - die meisten Datensets sind nicht gigantisch (median 100GB) und die wichtigsten Analysen erfolgen auf den jüngsten Daten, oft nur wenige hundert Megabytes.

Wie schon von Frank McSherry [McSherry] in 2015 demonstriert, kann man mit einer effizienten Implementierung unter Ausnutzung all der mächtigen Eigenschaften moderner CPUs (Vektorisierung, massives Pipelining, sehr große CPU Caches) und aktueller Algorithmen der Datenbankforschung große Datenmengen selbst auf einem einzelnen Thread verarbeiten.


Diesen Ansatz hat sich DuckDB auch zu eigen gemacht und ermöglicht damit effiziente, lokale Datenanalyse.

Aber genug der Einleitung, am besten machen wir uns gleich einmal mit DuckDB vertraut.
Keine Angst, die Ente beißt nicht.

Entweder per Paketmanager, oder Python Installer (pip) oder einfach per [Download] kann die kompakte Installation auf den eigenen Computer geholt werden (siehe Listing {counter:listing}). 
Aktuell ist gerade Version 0.8.0.

.Listing {listing} Installation
[source,shell]
----
apt install duckdb

# Alternativen
brew install duckdb
pip install duckdb
npm install duckdb
----

Danach ist die Kommandozeilenanwendung `duckdb`, aber auch die jeweiligen Bibliotheken verfügbar.
Wie schon erwähnt, gibt es keinen Datenbankserver, die Datenbank läuft direkt im Prozess der Anwendung.

Es gibt auch eine in-Browser [Web-Shell], die in einer WASM Sandbox läuft.

Netterweise kann man auch komplett ohne Persistenz, d.h. mit einer reinen Hauptspeicherdatenbank experimentieren.

Unser erstes Beispiel ist zwar ein "Hello World", aber wir steigen gleich auch weiter ein, mit Stringoperationen, transposition (unnest), Aggregation und Sortierung (siehe Listing {counter:listing}).

.Listing {listing} Hallo Welt!
[source,shell]
----
duckdb
-- Loading resources from /Users/mh/.duckdbrc
v0.8.0 e8e4cea5ec
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.

D select 'Hallo Welt!' as msg;
┌─────────────┐
│     msg     │
│   varchar   │
├─────────────┤
│ Hallo Welt! │
└─────────────┘

select split('Hallo Welt!','') as msg;
┌───────────────────────────────────┐
│                msg                │
│             varchar[]             │
├───────────────────────────────────┤
│ [H, a, l, l, o,  , W, e, l, t, !] │
└───────────────────────────────────┘

select unnest(split('Hallo Welt!','')) as c;
┌─────────┐
│    c    │
│ varchar │
├─────────┤
│ H       │
│ a       │
│ l       │
│ l       │
│ o       │
│         │
│ W       │
│ e       │
│ l       │
│ t       │
│ !       │
├─────────┤
│ 11 rows │
└─────────┘

select c, count(*) as freq from (
    select unnest(split('Hallo Welt!','')) as c
) group by c 
  order by freq desc limit 5;

┌─────────┬───────┐
│    c    │ freq  │
│ varchar │ int64 │
├─────────┼───────┤
│ l       │     3 │
│ H       │     1 │
│ a       │     1 │
│ o       │     1 │
│         │     1 │
└─────────┴───────┘
----

Für die Anzeige der Ergebnisse kann man zwischen verschiedenen Modi mittels `.mode <name>` wählen.
Wir haben bisher `duckbox` gesehen, es gibt auch `csv, json, jsoline, line` (jeder Wert auf einer neuen Zeile), `html, insert, trash` (keine Ausgabe) und andere mehr.
Andere nützliche Kommandos der CLI sind mittels `.help` verfügbar, `.timer on` gibt zum Beispiel die Laufzeit eines Statements aus.

Bei Fragen steht die umfangreiche und detaillierte Dokumentation [DuckDBDocs] zur Verfügung und eine sehr hilfsbereite Community beantwortet Fragen auf [Discord].

DuckDB unterstützt einen Großteil des SQL Standard, bringt zusätzlich noch viele nützliche Funktionen mit.

Sehr angenehm ist auch, dass die Datenbank relativ einfach mit weiterer Funktionalität erweitert werden kann.
Diese Erweiterungen werden der eigenen Installation mit `INSTALL name/url` und `LOAD name` hinzugefügt, und stehen ab dann allen APIs zur Verfügung.
Es gibt Erweiterungen für verschiedene Dateiformate und -quellen, Volltextsuche, Geodaten, und vieles mehr.

Wiederholte Konfiguration und Nutzung kann in `$HOME/.duckdbrc` abgelegt werden.

Ein sehr nützlicher Einsatzzweck von DuckDB ist die Analyse existierender Daten, die irgendwo in der Cloud via https oder Cloud Storage (S3, GCP, HDFS) zur Verfügung stehen, ohne dass man diese erst manuell herunterladen und importieren muss.

Desweiteren gibt es integrierte Unterstützung für CSV und eine Erweiterung für JSON und Parquet.
// Seit Version 0.8 sind viele dieser Operationen standardmäßig parallelisiert.

Damit können wir im nächsten Schritt gleich mal ein paar Daten aus dem Internet analysieren, z.B. Bevölkerungszahlen von Ländern [CSV] wie in Listing {counter:listing} zu sehen.

.Listing {listing}
[source,shell]
----
duckdb
INSTALL httpfs;
LOAD httpfs;

SELECT count(*) from 'https://github.com/bnokoro/Data-Science/raw/master/countries%20of%20the%20world.csv';
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│          227 │
└──────────────┘

-- mit read_csv_auto() gehen auch Shortlinks
SELECT * from read_csv_auto("https://bit.ly/3KoiZR0") LIMIT 2;
┌──────────────┬──────────────────────┬────────────┬───┬─────────────┬──────────┬─────────┐
│   Country    │        Region        │ Population │ … │ Agriculture │ Industry │ Service │
│   varchar    │       varchar        │   int64    │   │   varchar   │ varchar  │ varchar │
├──────────────┼──────────────────────┼────────────┼───┼─────────────┼──────────┼─────────┤
│ Afghanistan  │ ASIA (EX. NEAR EAS…  │   31056997 │ … │ 0,38        │ 0,24     │ 0,38    │
│ Albania      │ EASTERN EUROPE    …  │    3581655 │ … │ 0,232       │ 0,188    │ 0,579   │
├──────────────┴──────────────────────┴────────────┴───┴─────────────┴──────────┴─────────┤
│ 2 rows                                                             20 columns (6 shown) │
└─────────────────────────────────────────────────────────────────────────────────────────┘


SELECT count(*) as countries, max(Population) as max_population, 
round(avg(cast("Area (sq. mi.)" AS decimal))) as avgArea 
from read_csv_auto("https://bit.ly/3KoiZR0");

+-----------+----------------+----------+
| countries | max_population | avgArea  |
+-----------+----------------+----------+
| 227       | 1313973713     | 598227.0 |
+-----------+----------------+----------+

// natürlich können wir auch temporäre Tabellen erzeugen und diese benutzen
CREATE TABLE largest as SELECT * FROM read_csv_auto("https://bit.ly/3KoiZR0") 
ORDER BY 'Area (sq. mi.)' DESC LIMIT 20;

// dann ist die Antwort instantan
SELECT count(*) as countries, max(Population) AS max_population, 
round(avg(CAST("Area (sq. mi.)" AS decimal))) AS avgArea 
FROM largest;
----

Die Integration zum Lesen und Schreiben verschiedener Datenformate ist wirklich beachtlich.
Neben CSV und JSON Dateien können auch SQLite und Postgres Datenbanken verarbeitet werden.
Besonders die Unterstützung von Parquet und Arrow ist weit gediehen, dort können Filter und Selektions-Prädikate von SQL schon in der Zugriffschicht ausgeführt, und somit die zu ladende Menge von Daten erheblich reduziert werden.

Ein weiterer praktischer Einsatzzweck ist die Kombination von Datenbereinigung und Formatkonvertierung.
So können zum Beispiel Daten aus JSON oder CSV gelesen und bereinigt werden und dann als Parquet abgespeichert.

////
Die Erweiterung zur JSON Unterstützung ist sehr praktisch, so können Dateien oder API-Antworten aus einem JSON Objekt direkt in Tabellenzeilen umgewandelt werden, wie in Listing {counter:listing} ersichtlich.

.Listing {listing}
[source,sql]
----
select * from read_json('https://api.stackexchange.com/2.2/questions?pagesize=10&order=desc&sort=creation&tagged=duckdb&site=stackoverflow&filter=!5-i6Zw8Y)4W7vpy91PMYsKM-k9yzEsSC1_Uxlf',auto_detect=true, compression=gzip);

Error: Invalid Error: IO Error: HTTP GET error: Content-Length from server mismatches requested range, server may not support range requests.
----
////

=== Metadatenanalyse

DuckDB hilft uns auch dabei, Metadaten von Tabellen zu untersuchen (`describe`), und zu modifizieren, siehe Listing {counter:listing}.

Mit `read_csv_auto` bzw. `read_csv(AUTO_DETECT=true)` versucht DuckDB mittels einer Stichprobe die Datentypen der Spalten herauszufinden, fällt aber im Zweifelsfall auf Stringtypen `VARCHAR` zurück.

Ausser die Spalten `Country` und `Region` sollten aber alle anderen Spalten Integer- oder Dezimalzahlen sein.

Mittels `types={'spalte': 'typ'}` können selbst die Standard-SQL Typen angeben, die für spezifische Spalten genutzt werden sollen.

Man kann auch in eine existiernde Tabelle importieren, dann wird deren Schema genutzt: `COPY countries FROM 'countries of the world.csv' (AUTO_DETECT TRUE);`

// ALL_VARCHAR=TRUE
// SAMPLE_SIZE=-1
// IGNORE_ERRORS=TRUE // skip rows with dirty data
// columns={'Pop. Density (per sq. mi.)': 'decimal', ...}

.Listing {listing} Metadaten
[source,sql]
----
.mode duckbox
describe (select * from read_csv_auto("https://bit.ly/3KoiZR0"));
┌────────────────────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐
│            column_name             │ column_type │  null   │   key   │ default │  extra  │
│              varchar               │   varchar   │ varchar │ varchar │ varchar │ varchar │
├────────────────────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤
│ Country                            │ VARCHAR     │ YES     │         │         │         │
│ Region                             │ VARCHAR     │ YES     │         │         │         │
│ Population                         │ BIGINT      │ YES     │         │         │         │
│ Area (sq. mi.)                     │ BIGINT      │ YES     │         │         │         │
│ Pop. Density (per sq. mi.)         │ VARCHAR     │ YES     │         │         │         │
│ Coastline (coast/area ratio)       │ VARCHAR     │ YES     │         │         │         │
...
│ Climate                            │ VARCHAR     │ YES     │         │         │         │
│ Agriculture                        │ VARCHAR     │ YES     │         │         │         │
│ Industry                           │ VARCHAR     │ YES     │         │         │         │
│ Service                            │ VARCHAR     │ YES     │         │         │         │
├────────────────────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤
│ 20 rows                                                                        6 columns │
└──────────────────────────────────────────────────────────────────────────────────────────┘

.mode line
D select * from read_csv_auto("https://bit.ly/3KoiZR0") limit 1;
                           Country = Afghanistan 
                            Region = ASIA (EX. NEAR EAST)         
                        Population = 31056997
                    Area (sq. mi.) = 647500
        Pop. Density (per sq. mi.) = 48,0
      Coastline (coast/area ratio) = 0,00
                     Net migration = 23,06
Infant mortality (per 1000 births) = 163,07
                GDP ($ per capita) = 700
                      Literacy (%) = 36,0
                 Phones (per 1000) = 3,2
                        Arable (%) = 12,13
                         Crops (%) = 0,22
                         Other (%) = 87,65
                           Climate = 1
                         Birthrate = 46,6
                         Deathrate = 20,34
                       Agriculture = 0,38
                          Industry = 0,24
                           Service = 0,38

describe (select country, region, population, "Net migration", climate from 
    read_csv("https://bit.ly/3KoiZR0", auto_detect=true, header=true,
    types={'Climate':'float','Net migration':'float'}));
┌───────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐
│  column_name  │ column_type │  null   │   key   │ default │  extra  │
│    varchar    │   varchar   │ varchar │ varchar │ varchar │ varchar │
├───────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤
│ Country       │ VARCHAR     │ YES     │         │         │         │
│ Region        │ VARCHAR     │ YES     │         │         │         │
│ Population    │ BIGINT      │ YES     │         │         │         │
│ Net migration │ FLOAT       │ YES     │         │         │         │
│ Climate       │ FLOAT       │ YES     │         │         │         │
└───────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘

--- ALTER TABLE für Datentyp mit Ausdruck für Konvertierung
ALTER TABLE countries ALTER Climate SET DATA TYPE FLOAT USING CAST(Climate AS FLOAT);
----

DuckDB kennt einige zusätzliche Typen wie:

* Enums für abgezählte Werte
* Listen/Arrays
* Map für Schlüssel-Wert-Paare
* Structs für wiederkehrende Strukturen
* Date, Timestamp, Interval
* Bitstring
* Blob
* NULL
* Union (von Datentypen)

Es gibt natürlich auch "Meta"-Funktionien, mit denen man die Datenbank selbst inspizieren kann, hier sind einige davon aufgelistet, mittels `select function_name from duckdb_functions() where function_name like 'duckdb_%';`.
Für den SQL-Standard sind einige davon auch als im `information_schema` Schema als Tabellen verfügbar.

* duckdb_keywords()
* duckdb_types()
* duckdb_functions()
* duckdb_databases()
* duckdb_schemas() - `information_schema.schemata`
* duckdb_tables() - `information_schema.tables`
* duckdb_views()
* duckdb_sequences()
* duckdb_constraints()
* duckdb_indexes()
* duckdb_columns() - `information_schema.columns`
* duckdb_settings()
* duckdb_extensions()
* current_schema()
* current_schemas()

=== Test mit größeren Datenmengen - Stackoverflow Dump

Um DuckDB mit größeren Datenmengen zu testen, habe ich den aktuellen Dump von Stackoverflow [StackOverflow-Dump] heruntergeladen und mit meinem [Xml-Converter-Tool] nach CSV gewandelt, da ich keine XML Erweiterung für DuckDB gefunden habe.

// TODO Parquet
Es sind zwar nur 65000 Tags und 20 Millionen Nutzer (2.2 GB CSV), aber 58 Millionen Posts (5.3 GB CSV), so dass sich das schon mal lohnt.
// 58 329 358 posts 5.278.202.143 b
In Listing {counter:listing} ist zu sehen, wie wir die Daten lesen, in Tabellen konvertieren und dann analysieren können.

////
create table users as (
select * from read_csv_auto("so/Users.csv.gz",auto_detect=true, 
column_names=['id','name','reputation','createdAt','accessedAt',
'url','location','views','upvotes','downvotes','age','accountId'])
);

select name, reputation, today()-createdAt as age, createdAt, accountId, upvotes, downvotes
from users where reputation > 1000000 order by age asc;
┌─────────────────┬────────────┬─────────────────────────┬───────────┬─────────┬───────────┐
│      name       │ reputation │        createdAt        │ accountId │ upvotes │ downvotes │
│     varchar     │   int64    │        timestamp        │   int64   │  int64  │   int64   │
├─────────────────┼────────────┼─────────────────────────┼───────────┼─────────┼───────────┤
│ VonC            │    1194435 │ 2008-09-13 22:22:33.173 │      4243 │   68498 │       405 │
│ Jon Skeet       │    1389256 │ 2008-09-26 12:05:05.15  │     11683 │   17135 │      8011 │
│ Marc Gravell    │    1009857 │ 2008-09-29 05:46:02.697 │     11975 │   27390 │      1129 │
│ Darin Dimitrov  │    1014014 │ 2008-10-19 16:07:47.823 │     14332 │    1949 │      2651 │
│ Martijn Pieters │    1016741 │ 2009-05-03 14:53:57.543 │     35417 │    5851 │     22930 │
│ T.J. Crowder    │    1010006 │ 2009-08-16 11:00:22.497 │     52616 │   14819 │     34259 │
│ BalusC          │    1069162 │ 2009-08-17 16:42:02.403 │     52822 │   15829 │     23484 │
│ Gordon Linoff   │    1228338 │ 2012-01-11 19:53:57.59  │   1165580 │   20567 │        42 │
└─────────────────┴────────────┴─────────────────────────┴───────────┴─────────┴───────────┘

select name, reputation, reputation/day(today()-createdAt) as rate, today()-createdAt as age, 
       createdAt, accountId, upvotes, downvotes
from users where reputation > 1000000 order by rate desc;

todo per year, pivot, window
////

.Listing {listing} Stackoverflow Analyse
[source,sql]
----
duckdb stackoverflow.db

select name, count 
from read_csv('so/Tags.csv.gz',column_names=['name','count','id'],auto_detect=true)
order by count desc limit 5;

┌────────────┬─────────┐
│    name    │  count  │
│  varchar   │  int64  │
├────────────┼─────────┤
│ javascript │ 2479947 │
│ python     │ 2113196 │
│ java       │ 1889767 │
│ c#         │ 1583879 │
│ php        │ 1456271 │
└────────────┴─────────┘

create table tags as select name, count 
from read_csv('so/Tags.csv.gz',column_names=['name','count','id'],auto_detect=true);

create table users as (
select * from read_csv_auto("so/Users.csv.gz",auto_detect=true, 
column_names=['id','name','reputation','createdAt','accessedAt',
'url','location','views','upvotes','downvotes','age','accountId'])
);

select count(*) from users; // 19942787

.timer on

SELECT name, reputation, round(reputation/day(today()-createdAt)) as rate, day(today()-createdAt) as days, 
       createdAt, accountId, upvotes, downvotes
FROM users WHERE reputation > 1000000 ORDER BY rate DESC;

┌─────────────────┬────────────┬────────┬───────┬───┬───────────┬─────────┬───────────┐
│      name       │ reputation │  rate  │ days  │ … │ accountId │ upvotes │ downvotes │
│     varchar     │   int64    │ double │ int64 │   │   int64   │  int64  │   int64   │
├─────────────────┼────────────┼────────┼───────┼───┼───────────┼─────────┼───────────┤
│ Gordon Linoff   │    1228338 │  296.0 │  4154 │ … │   1165580 │   20567 │        42 │
│ Jon Skeet       │    1389256 │  259.0 │  5356 │ … │     11683 │   17135 │      8011 │
│ VonC            │    1194435 │  222.0 │  5369 │ … │      4243 │   68498 │       405 │
│ BalusC          │    1069162 │  213.0 │  5031 │ … │     52822 │   15829 │     23484 │
│ T.J. Crowder    │    1010006 │  201.0 │  5032 │ … │     52616 │   14819 │     34259 │
│ Martijn Pieters │    1016741 │  198.0 │  5137 │ … │     35417 │    5851 │     22930 │
│ Darin Dimitrov  │    1014014 │  190.0 │  5333 │ … │     14332 │    1949 │      2651 │
│ Marc Gravell    │    1009857 │  189.0 │  5353 │ … │     11975 │   27390 │      1129 │
├─────────────────┴────────────┴────────┴───────┴───┴───────────┴─────────┴───────────┤
│ 8 rows                                                          8 columns (7 shown) │
└─────────────────────────────────────────────────────────────────────────────────────┘
Run Time (s): real 0.006 user 0.007980 sys 0.001260

WITH top_users as select ...
SELECT name, reputation, rate, bar(rate,150,300) AS bar FROM top_users;
┌─────────────────┬────────────┬────────┬──────────────────────────────────────────────────────────────┐
│      name       │ reputation │  rate  │                             bar                              │
│     varchar     │   int64    │ double │                           varchar                            │
├─────────────────┼────────────┼────────┼──────────────────────────────────────────────────────────────┤
│ Gordon Linoff   │    1228338 │  296.0 │ ██████████████████████████████████████████████████████████…  │
│ Jon Skeet       │    1389256 │  259.0 │ ██████████████████████████████████████████████████████████▏  │
│ VonC            │    1194435 │  222.0 │ ██████████████████████████████████████▍                      │
│ BalusC          │    1069162 │  213.0 │ █████████████████████████████████▌                           │
│ T.J. Crowder    │    1010006 │  201.0 │ ███████████████████████████▏                                 │
│ Martijn Pieters │    1016741 │  198.0 │ █████████████████████████▌                                   │
│ Darin Dimitrov  │    1014014 │  190.0 │ █████████████████████▎                                       │
│ Marc Gravell    │    1009857 │  189.0 │ ████████████████████▊                                        │
└─────────────────┴────────────┴────────┴──────────────────────────────────────────────────────────────┘
Run Time (s): real 0.001 user 0.000374 sys 0.000069

create table posts as (
select * from read_csv_auto("so/Posts.csv.gz",auto_detect=true, 
column_names=['id','title','postType','createdAt','score',
'views','answers','comments','favorites','updatedAt'])
);
Run Time (s): real 38.985 user 327.515702 sys 24.987078

select count(*) from posts;
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│     58329356 │
└──────────────┘
Run Time (s): real 0.010 user 0.000000 sys 0.162503

select year(createdAt) as year, avg(views), max(answers), max(comments) 
from posts 
group by year order by year desc limit 10;
┌───────┬────────────────────┬──────────────┬─────────────────┐
│ year  │    avg("views")    │ max(answers) │ max("comments") │
│ int64 │       double       │    int64     │      int64      │
├───────┼────────────────────┼──────────────┼─────────────────┤
│  2023 │  44.38945117445532 │           15 │              69 │
│  2022 │  265.4586339123072 │           44 │              73 │
│  2021 │  580.1325887811724 │           65 │              80 │
│  2020 │  846.6885113285923 │           59 │              74 │
│  2019 │ 1189.7090769531437 │           60 │              62 │
│  2018 │  1647.557730647355 │          121 │              65 │
│  2017 │ 1993.5133771973378 │           65 │             110 │
│  2016 │ 2201.9238769664453 │           74 │             135 │
│  2015 │  2349.146274062714 │           82 │             115 │
│  2014 │ 2841.1271646657733 │           92 │             107 │
├───────┴────────────────────┴──────────────┴─────────────────┤
│ 10 rows                                           4 columns │
└─────────────────────────────────────────────────────────────┘
Run Time (s): real 0.038 user 4.131562 sys 0.014536
----

Wie wir sehen, bringen auch mittelgroße Datenmengen DuckDB nicht aus dem Gleichgewicht, für Tests mit Millarden von Datensätzen wird oft der [New-York-Taxi] Datensatz benutzt, der im Parquet Format vorliegt.

// TODO pivot, ...

Da CSV schon etwas in die Jahre gekommen ist, können die Daten auch nach Parquet exportieren, ein modernes Format für die analytische Datenverarbeitung (Listing {counter:listing}).
Für die 20M Nutzer dauert es 5 Sekunden bis die 10 Dateien mit 1G geschrieben sind.
Das Lesen der Dateien ist jetzt viel schneller als von CSV.

.Listing {listings} - Parquet Dateien schreiben
[source,sql]
----
.timer on
COPY (SELECT * FROM users ORDER BY accessedAt DESC) TO 'users.parquet' 
     (FORMAT PARQUET, PER_THREAD_OUTPUT TRUE);
100% ▕████████████████████████████████████████████████████████████▏ 
Run Time (s): real 5.244 user 19.425849 sys 9.041617

ls users.parquet 
data_0.parquet	data_2.parquet	data_4.parquet	data_6.parquet	data_8.parquet
data_1.parquet	data_3.parquet	data_5.parquet	data_7.parquet	data_9.parquet
mh@Ombatis Downloads % du -sh users.parquet
954M	users.parquet

select count(*) from read_parquet('users.parquet/*');
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│     19942787 │
└──────────────┘
Run Time (s): real 0.014 user 0.018494 sys 0.006188

select count(*) from read_csv_auto('so/Users.csv.gz');
100% ▕████████████████████████████████████████████████████████████▏
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│     19942787 │
└──────────────┘
Run Time (s): real 7.040 user 16.688485 sys 0.173113
----

=== DuckDB und Python

Ein sehr praktischer Aspekt von DuckDB ist die Nutzung innerhalb von Python Datenanalyse-Prozessen und -Notebooks.

Daten die in Pandas Dataframes vorliegen, können direkt und ohne Transformation oder Kopiervorgang von DuckDB genutzt werden.
Ergebnisse von DuckDB werden ebenso als Dataframes bereitgestellt, und können dann mit den gängigen Bibliotheken weiterverarbeitet werden (Listing {counter:listing}).

.Listing {listing} - Nutzung mit Python
[source,python]
----
import duckdb
import pandas as pd

con = duckdb.connect(database='stackoverflow.db', read_only=True)
tags_df = con.execute("""select * from tags""").df()

tags_df.head()
         name    count
0        .net   329455
1        html  1167742
2  javascript  2479947
3         css   787138
4         php  1456271

con.query("select count(*) from tags_df")
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│        64465 │
└──────────────┘
----

Für die Visualisierung von Ergebnissen können dank der transparenten Pandas Integration existierende Bibliotheken wie matplotlib genutzt werden.

Für interaktive Analyse-Anwendungen integriert es sich auch gut mit Streamlit, wie auf [LDWM-Streamlit] zu sehen.

DuckDB stellt auch eine fluent "relational API" [PythonDSL] bereit, die statt SQL eingesetzt werden kann und die Wiederverwendung von "Relations", sowie Setoperationen, Filter, Projektionen, Aggregationen usw. unterstützt, siehe Listing {counter:listing}.
Als Quelle für initiale "Relationen" können neben SQL-Statements auch Daten direkt aus Parquet, Arrow und CSV Dateien gelesen werden.

Mir persönlich ist die DSL nicht weit genug entwickelt, da immer noch SQL Fragemente als Parameter übergeben werden müssen.

.Listing {listing} - relational API in Python
[source,python]
----
import duckdb
import pandas as pd

con = duckdb.connect(database='stackoverflow.db', read_only=True)

rel = con.sql('SELECT * FROM users')
rel = rel.filter('reputation > 1000')
rel = rel.aggregate('year(createdAt) as year, count(*) as activeUsersPerYear')
rel = rel.order('year DESC').limit(10)
rel.show()

┌───────┬────────────────────┐
│ year  │ activeUsersPerYear │
│ int64 │       int64        │
├───────┼────────────────────┤
│  2023 │                  2 │
│  2022 │                251 │
│  2021 │                757 │
│  2020 │               1752 │
│  2019 │               2749 │
│  2018 │               4601 │
│  2017 │               7606 │
│  2016 │              11963 │
│  2015 │              16508 │
│  2014 │              21886 │
├───────┴────────────────────┤
│ 10 rows          2 columns │
└────────────────────────────┘
----

Skalare Python Funktionen können seit Version 0.8 mittels `duckdb.create_function('name', funktion, parameter-typen, return-typ)` in der Datenbank registriert und benutzt werden.

////
[source,python]
----
----
////

=== Nutzung mit Java

Ähnlich wie in Python ist die Nutzung von DuckDB in Java erfreulich unkompliziert.

Der JDBC Treiber ist auf Maven verfügbar und führt die Datenbank auch wieder innerhalb unseres Prozesses aus.

In Listing {counter:listing} ist ein kleines JBang Beispiel zu sehen, dass die Verbindung zur Datenbank öffnet, die übergebene SQL Abfrage ausführt und die Ergebnisse als Ascii-Tabelle darstellt.

.Listing {counter:listing} - Nutzung von Java mittels JDBC und JBang
[source,java]
----
///usr/bin/env jbang "$0" "$@" ; exit $?
//DEPS org.duckdb:duckdb_jdbc:0.8.0
//DEPS com.github.freva:ascii-table:1.2.0
//DEPS org.apache.commons:commons-lang3:3.0

import static java.lang.System.*;
import java.sql.*;
import java.util.*;
import com.github.freva.asciitable.*;

public class DuckDB {

    public static void main(String... args) throws Exception {
        try (Connection con=DriverManager.getConnection(getenv("JDBC_URL"));
             Statement stmt=con.createStatement();
             ResultSet rs=stmt.executeQuery(String.join(" ",args))) {
                ResultSetMetaData meta=rs.getMetaData();
                String[] cols=new String[meta.getColumnCount()];
                for (int c=1;c<=cols.length;c++) 
                    cols[c-1]=meta.getColumnName(c);
                int row=0;
                String[][] rows=new String[100][];
                while (rs.next() || row>=rows.length) {
                    rows[row]=new String[cols.length];
                    for (int c=1;c<=cols.length;c++) 
                        rows[row][c-1]=rs.getString(c);
                    row++;
                }
                out.println(AsciiTable.getTable(cols, Arrays.copyOf(rows,row)));
             }
    }
}

export JDBC_URL="jdbc:duckdb:stackoverflow.db"
jbang DuckDB.java "SELECT name, reputation FROM users ORDER BY reputation DESC LIMIT 5"

+-----------------+------------+
| name            | reputation |
+-----------------+------------+
|       Jon Skeet |    1389256 |
+-----------------+------------+
|   Gordon Linoff |    1228338 |
+-----------------+------------+
|            VonC |    1194435 |
+-----------------+------------+
|          BalusC |    1069162 |
+-----------------+------------+
| Martijn Pieters |    1016741 |
+-----------------+------------+
----

=== Implementierungsdetails und Architektur

Wie einer, an einem Datenbanklehrstuhl entwickelten Datenbank würdig, nutzt DuckDB alle relevanten Mechanismen moderner OLAP Datenbanken.
Wegen der zumeist eingebetteten Ausführung können keine komplexen Bibliotheken oder Infrastrukturen genutzt werden, da diese meist nicht portabel sind, bzw. Betriebssystem-Signale benötigen, oder im Ernstfall den Prozess beenden.
Daher sind auch effizientes Resourcenmanagement und Datenzugriff möglichst ohne Speicherkopien wichtig.

Für Endanwendungen wird neben der C/C++ API auch Integration für Python und R vom Kernsystem bereitgestellt, andere Bibliotheken nutzen die C/C++ API.
Als Parser wird ein modifizierter Postgres-Parser genutzt, der sehr flexible auf die Bedürfnisse angepasst werden kann.
Abfrageplannung erfolgt in einem zumeist kostenbasierten Optimizer, der Ansätze wie "join-order-optimization" und "dynamic programming" ausnutzt.
// , mit Optimierung von JOIN Reihenfolgen und dynamischer Programmierung ().
Für Indizes und Constraints (PK, FK), sowie Geo und Range-Abfragen sowie Joins benutzt DuckDB Adaptive Radix Tree (ART) Indizes (Tries mit horizontaler und vertikaler Kompression).
// https://duckdb.org/2022/07/27/art-storage.html
// trees that also contain the data (e.g. 1 char or 1 byte per level)
// vertical compression for nodes with only one child -> radix tree -> store prefix and then only next child which has bifurcation
// horizontal compresion -> ART -> on each position of the 256 values of a byte there is one pointer pointing down (or null) -> 
Die Ausführung des physischen Plans übernimmt eine vektorisierte, kolumnare, parallele Implementierung, die auf Teilmengen (Batches) von Daten arbeitet (Morsel Ansatz) und damit eine gute Balance zwischen der Verarbeitung pro Zeile bzw. der kompletten Daten auf einmal erreicht.
// vectorized push based model, vectors flow through the operators
Alle Daten innerhalb von DuckDB liegen in getypten, optimierten Vektor-Implementierungen für verschiedene Inhalte (numerisch-Felder, Konstanten, Strings, Dictionary-Lookups, Listen, Structs, usw.) die sowohl durch Kompression, Metadaten (min, max) und zusätzliche Indizes die Auswahl bzw. Verarbeitung beschleuinigen.
Diese Vektoren implementieren alle relationalen Operationen in C++ Klassen mittes Templates für die verschiedenen Datentypen.
Im Ablauf werden die Vektoren durch die Plan-Operatoren von einem zum anderen weitergereicht (push-based).

DuckDB ist auch transaktional, damit während der analytischen Abfragen auch Updates der darunterliegenden Daten erfolgen können.
Es benutzt eine OLAP optimierte Variante MVCC (Multi Version Concurrency Control) mit serialiserten Transaktionen wie auch das HyPer System der TU München.
Dabei werden Aktualisierungen direkt ausgeführt und vorherige Werte in einem Undo-Puffer gehalten, falls die Transaktion zurückgerollt werden muss.

Alles in allem laut Aussage der Entwickler eine Lehrbuch-Architektur, aber das Datenbank-sLehrbuch ist dann schon ziemlich modern.

// SIGMOD 2019 paper https://hannes.muehleisen.org/publications/SIGMOD2019-demo-duckdb.pdf
// Dissecting DuckDB: The internals of the “SQLite for Analytics” https://pdet.github.io/assets/papers/duck_sbbd.pdf 
// DuckDB an Embeddable Analytical RDBMS https://db.in.tum.de/teaching/ss19/moderndbs/duckdb-tum.pdf

// DuckDB ist in C++ 11 geschrieben und maximiert ... TODO

////

* Co designed with Velox 
* Similar to Arrow but designed for execution, not storage/streaming
* ART index, used also for maintaining key constraints 
* Combination of both cost/rule based optimizer
* vectorization / SIMD
* morsel driven parallelism
* batching
* zero memory copy / data sharing
* transactional (concurrent updates during analytics queries)
* effizientes resourcenmanagement
* keine crashes erlaubt, da in-prozess
* no dependencies (they often use signal handlers, or exit the process)

textbook implementation
-> Parser, logical planner, optimizer, physical planner, execution engine.
-> orthogonal: transaction and storage manager

API C/C++/SQLite  [2]
SQL Parser libpg_query
Optimizer   Cost-Based
Execution Engine Vectorized
Concurrency Control Serializable MVCC
Storage DataBlocks
* larger than memory execution (out-of-core)
    * special sort,join,window operators
    * streaming engine
    * graceful performance degradation
    * don't fully switch over to disk based execution
    * only write minimally to disk
    * batched vectors better suited
* single file block based storage
    * WAL separate file
    * ACID with headers -> versions
    * fixed sized blocks (64k)
    * tables partitioned into row groups, 120k rows, 
    * row groups are the parallelism and checkpoint unit
    * rewrite single grow group
    * distribute row groups over threads
    * column storage
    * compression, speeds up IO
    * generalized lz4, zstd
    * specified RLE, bitpacking, dict, frame of reference, chimps, FOR, FSST, 
    * based on patterns in data
    * different algorithms pick based on data in column per row group
    * no compaction yet, but block reusage
* UDFs in Python, tbd JS + WASM + extensions
* extensions as shared libraries
* geo extension
* pluggable filesystem (httpfs etc.)
* pluggable catalogue - custom like sqlite extension
* 
* lock free buffer mgr, inspired by lean-store, sort of LRU
* pin blocks, fix them in memory
* lean-store - as few centralized datastructures as possible
* vectorized push based model, vectors flow through the operators
* Morsel push based model, parallelism aware operators
* model control flow outside of operators in central location
* state explicit
* vectors cache between operators
* query cancellation - vectors out of / into operator -> point of cancelation

* allows to pause execution
* buffers full - pause pipeline
* source, intermediate, sink operator, model parallelism in source/sink
* global / local state
* columnar-vectorized engine
* vector based chunk processing, not row based or bulk
* state of the art methods and algorithms
* API - C/C++/SQLite
* SQLite compatibility layer through re-linking or library overloading
* R and Python APIs
* stripped down, flexible postgres parser -> parse tree of C structures -> c++ classes
* logical planner -> binder (schema object binding) + plan generator (parse-tree -> logical operators)
* database statistics propagated through the different expression trees as part of the planning process. 
* These statistics are used in the opti- mizer itself, and are also used for integer overflow prevention by upgrading types when required.
* DuckDB’s optimizer performs join order optimization us- ing dynamic programming [7] with a greedy fallback for complex join graphs [11]. It performs flattening of arbitrary subqueries as described in Neumann et al. [9]. 
* In addition, there are a set of rewrite rules that simplify the expression tree, by performing e.g. common subexpression elimination
* -> optimized logical plan
* -> physical plan ()
* vectorized  vectorized interpreted execution engine -> not JIT because fewer dependencies/portability (no LLVM)
* numerics -> native array
* strings -> extra heap
* null bit-vector faster for intersection
*  selection vector, which is a list of offsets into the vector stating which indices of the vector are relevant
* built in templated vector library for all the relational operators
* started out as pull based system (Vector Volcano model)
    * pull chunks from root node -> subsequent pulls until leaf nodes with tables scan etc. reached
    * nice and easy but doesn't work well for multithreaded
    * control inside of operators
    * but larges stack size
* MVCC serializable from HyPer -> for hybrid OLTP/OLAP systems
* snapshot isolation
* abort tx when multiple changes to same row
* 
* this variant updates data in-place immediately, and keeps previous states stored in a separate undo buffer for concurrent transactions and aborts.
* concurrent modification is frequently requested
* persistence ead-optimized DataBlocks storage layout 
* logical tables -> light-weight compressed chunks of columns min/max metadata for each block 
* + additional lightweight index for each column
* large datasets on restricted hardware
* benefits of embedded operations
* MonetDBLite begins to suf- fer from excessive intermediate result materialization due to its bulk processing model. 
* HyPer is extremely fast in processing queries, it will not be able to transfer result sets as quickly as DuckDB using its socket client protocol
*  DuckDB already supports inter-query parallelism but intra-query parallelism will be added as well.
* Vector is the container format used to store in-memory data during execution.
* DataChunk is a collection of Vectors, used for instance to represent a column list in a PhysicalProjection operator.
* different types of vectors based on contained data
* optimized operations between vector types for operations (e.g. constant times flat vector)
* 
* https://duckdb.org/internals/vector
* All operators in DuckDB are optimized to work on Vectors of a fixed size (default 2048 tuples)
* different physical representations per type incl. compression + compressed execution
* flat vectors (arrays), constant vectors (one value), 
* Dictionary-Lookups - dict-vectors (child-dict + selection vector - index into dict)
* sequence - base + increment
* 16 bytes -> short strings < 12 bytes inlined (like Umbra), otherwise length + pointer (8) + short(4) prefix for fast initial comparison
* list vectors with child vector + offset/indexes
* struct + map (LIST[STRUCT(key KEY_TYPE, value VALUE_TYPE)]) + union vectors  (UNION utilizes the same structure as a STRUCT)
* nested types recursively as vectors 
* structs -> each entry as it's own vector (column)
* lists: offset + length vector into a child vector of the whole list (allow sublists etc)
* no structs containing structs (yet)
* comb. explosion on vector combinations in operators -> flatten/decompresss into flatvector => data copy / move + expansion
* instead unified format/view -> indexed lookup array -> into the original vector 
    * reusable fixed size array for constant vector
    * for dict-lookup already lookup vector into dictionary
    * no system penalty
* vectors sources
* always specialize on constant vector
* not-vectorized storage (but compatible) - bit-packing
* compressed storage
* at storage layer -> determine which vector type (based on actually stored data)
* no recompression



=== Erweiterungen

Erweiterungen:
* Spatial PostGeese
* FTS

////

=== Erweiterte Funktionen - PV Analyse

Auch SQL Experten werden nicht enttäuscht, neben voller Unterstützung von Window Funktionen `(OVER ... PARTITION BY)`, `PIVOT` sind auch Common Table Expressions in DuckDB an der Tagesordnung.

Mein Kollege Michael Simons, der auch schon öfter in Autor dieser Kolumne war, hat DuckDB genutzt, um die Erzeugungs- und Verbrauchsdaten seiner nagelneuen Photovoltaik Anlage zu analysieren [SimonsPV].

Dabei war die Extraktion der Daten aus der Anbietersoftware der größte Aufwand, zwei interessante Beispielabfragen sind in Listing {counter:listing} und {counter:listing} zu sehen.

.Listing 11 - PV Analyse
[source,sql]
----
WITH production_per_month_and_hour AS (
        SELECT any_value(strftime(measured_on, '%B'))    AS Month,
               any_value(date_part('hour', measured_on)) AS Hour,
               avg(production) / 1000                    AS Energy
          FROM measurements
         GROUP BY date_trunc('hour', measured_on)
         ORDER BY Hour
    )
SELECT *
FROM production_per_month_and_hour
PIVOT (
    round(avg(Energy), 2)
    FOR Month IN ('January', 'February', 'March', 'April', 'May', 'June')
    GROUP BY Hour
);
----

.Listing {listing} Ausgabe PV Analyse
----
┌───────┬─────────┬──────────┬────────┬────────┬────────┬────────┐
│ Hour  │ January │ February │ March  │ April  │  May   │  June  │
│ int64 │ double  │  double  │ double │ double │ double │ double │
├───────┼─────────┼──────────┼────────┼────────┼────────┼────────┤
│     0 │         │          │        │    0.0 │    0.0 │        │
│     1 │         │          │        │    0.0 │    0.0 │        │
│     2 │         │          │        │    0.0 │    0.0 │        │
│     3 │         │          │        │    0.0 │    0.0 │        │
│     4 │         │          │        │    0.0 │    0.0 │        │
│     5 │         │          │        │    0.0 │    0.0 │        │
│     6 │         │          │        │   0.05 │   0.36 │        │
│     7 │         │          │        │   0.57 │   1.23 │        │
│     8 │         │          │        │   1.37 │   2.16 │        │
│     9 │         │          │        │   2.18 │   2.99 │        │
│    10 │         │          │        │   2.31 │   3.69 │        │
│    11 │         │          │        │   2.28 │   3.92 │        │
│    12 │         │          │        │   3.14 │    4.3 │        │
│    13 │         │          │        │   3.13 │   4.36 │        │
│    14 │         │          │        │   2.89 │   4.26 │        │
│    15 │         │          │        │   2.82 │   3.85 │        │
│    16 │         │          │        │   1.87 │   3.13 │        │
│    17 │         │          │        │   1.41 │   2.51 │        │
│    18 │         │          │        │   1.05 │   1.86 │        │
│    19 │         │          │        │   0.55 │   1.22 │        │
│    20 │         │          │        │   0.08 │   0.43 │        │
│    21 │         │          │        │    0.0 │   0.01 │        │
│    22 │         │          │        │    0.0 │    0.0 │        │
│    23 │         │          │        │    0.0 │    0.0 │        │
├───────┴─────────┴──────────┴────────┴────────┴────────┴────────┤
│ 24 rows                                              7 columns │
└────────────────────────────────────────────────────────────────┘
----

////
.Bild 2 - Ausgabe PV Analyse
image::pv_pivot.png[]

.Listing {listing} - Tages Statistik
[source,sql]
----
WITH per_day AS (
    SELECT sum (power) / 4 / 1000 AS V
    FROM production
    GROUP BY date_trunc('day', measured_on)
)

SELECT 
    round (min(v), 2) AS 'Worst day',
    round (max (v), 2) AS 'Best day',
    round (avg (v), 2) AS 'Daily Average'
    round (median (v), 2) AS 'Median',
    round (sum (v), 2) AS 'Total production'
FROM per_day;
----

.Bild 3 - Ausgabe Tagesstatistik
image::pv_per_day.png[]
////

=== Anwendungsfälle

Für ein Tool wie DuckDB gibt es viele Anwendungsfälle, am spannendsten ist es natürlich wenn es in existierende Cloud, Mobile, Desktop und Kommandozeilenanwendungen integriert werden kann und hinter den Kulissen seinen Dienst versieht.

Gerade für die Analyse von Daten, die das eigene Gerät nicht verlassen sollen, wie Gesundheits-, Trainings-, Finanz- oder Heimautomatisierungs-Daten bietet sich eine effiziente lokale Infrastruktur an.

Aber auch für die schnelle Analyse größerer Datenmengen, wie z.B. Logdateien, bei der die Berechnung und Reduktion dort erfolgen kann, wo die Daten gespeichert sind und damit hohe Datentransfer(kosten) gespart werden ist DuckDB nützlich.

Für Data Scientists kann Datenaufbereitung, Analyse, Filterung und Aggregation effizienter als mit Pandas erfolgen, ohne die bequeme Umgebung eines Notebooks mit Python- oder R-APIs zu verlassen.

Spannend wird auch die verteilte Analyse von Daten, je nach Menge, Speicherort, und Anwendungsfall, die zum Beispiel von [Motherduck] zwischen Cloud-Speicher, Edge-Netzwerk und lokalem Gerät balanciert wird.

=== Fazit

DuckDB ist ein erfrischend praktischer Ansatz für die effiziente Datenanalyse.

Neben dem großen Funktionsumfang, nahtloser Integration, guter Dokumentation, hilfsbereiter Community und schnellem Einstieg ist auch die kontinuierliche Weiterentwicklung durch Datenbankforscher von Weltrang ein Garant für eine erfolgreiche Zukunft.

Quack!

== Resourcen

* [DuckDB Docs] https://duckdb.org/docs/
* [DuckDB JSON] https://duckdb.org/docs/extensions/json.html
* [DuckDB Discord] https://discord.duckdb.org/
* [Web-Shell] https://shell.duckdb.org/
* [Download] https://github.com/duckdb/duckdb/releases
* [LDWM] Learn Data With Mark https://youtube.com/@learndatawithmark
* [LDWM-Streamlit] https://www.youtube.com/watch?v=65MoH1rlK7E&list=PLw2SS5iImhEThtiGNPiNenOr2tVvLj6H7&index=15
* [SimonsPV] https://github.com/michael-simons/pv
* [StackOverflow-Dump] https://archive.org/download/stackexchange
* [Xml-Converter-Tool] https://github.com/neo4j-examples/neo4j-stackoverflow-import
* [SQLitePodcast] https://corecursive.com/066-sqlite-with-richard-hipp/
* [McSherry] http://www.frankmcsherry.org/graph/scalability/cost/2015/01/15/COST.html
* [BigData] https://motherduck.com/blog/big-data-is-dead/
* [Motherduck] https://motherduck.com
* [PythonDSL] https://duckdb.org/docs/api/python/relational_api
// * [DuckDBFTS] https://duckdb.org/2021/01/25/full-text-search.html
// * [PostGeese]
* [New-York-Taxi] https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
* [InternalsVideo] https://www.youtube.com/watch?v=bZOvAKGkzpQ